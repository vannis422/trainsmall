{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmp0Etd8bJrofPDX3LDjNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f19fb44e961a4b388570b4ee4454554e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a23bb55d0e884cb3a7f43f7b8c5a02bb",
              "IPY_MODEL_7ea2396af1284336b5f3ca96108b1d66",
              "IPY_MODEL_d809454885f74cce93deca42cf220131"
            ],
            "layout": "IPY_MODEL_332dc2fe7865479fafcbbb6b2045aca8"
          }
        },
        "a23bb55d0e884cb3a7f43f7b8c5a02bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a52de44af414b378c7e62d43d3bcb75",
            "placeholder": "​",
            "style": "IPY_MODEL_df5164cb363a466d826e2cff5f046273",
            "value": "Fetching 13 files: 100%"
          }
        },
        "7ea2396af1284336b5f3ca96108b1d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f73a0067b51c4726845b9650aa14303a",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c4ed871a1d4ecb80857d6fa1d4ad73",
            "value": 13
          }
        },
        "d809454885f74cce93deca42cf220131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a2c6570f55d4f5490a52e70f984b167",
            "placeholder": "​",
            "style": "IPY_MODEL_67be02c54fa845108eb3d4812a79f4a5",
            "value": " 13/13 [00:00&lt;00:00, 467.99it/s]"
          }
        },
        "332dc2fe7865479fafcbbb6b2045aca8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a52de44af414b378c7e62d43d3bcb75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df5164cb363a466d826e2cff5f046273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f73a0067b51c4726845b9650aa14303a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c4ed871a1d4ecb80857d6fa1d4ad73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a2c6570f55d4f5490a52e70f984b167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67be02c54fa845108eb3d4812a79f4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6e1202a5bde498fbc668a608e4397f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de8b9fe6efd44e4ebdb1a1606692e7ed",
              "IPY_MODEL_dd479ca6167e4a7f85b2dc1f4f0f4401",
              "IPY_MODEL_e2401253fb464a6497c621e18777ade4"
            ],
            "layout": "IPY_MODEL_e32a3a178cf642219da252343c2a189e"
          }
        },
        "de8b9fe6efd44e4ebdb1a1606692e7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_082bf1d4d46a43e598380be9c8d85f93",
            "placeholder": "​",
            "style": "IPY_MODEL_647ea4e598694e3582e45e3f8cf5ea41",
            "value": "Map: 100%"
          }
        },
        "dd479ca6167e4a7f85b2dc1f4f0f4401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23caab86142b472eb55f11714ba1be74",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9492a19261a34ff7aaf17acc5b6e4d62",
            "value": 200
          }
        },
        "e2401253fb464a6497c621e18777ade4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8248344165ca4294bdce77255215d99b",
            "placeholder": "​",
            "style": "IPY_MODEL_e462e1e042764266a32160381a599db8",
            "value": " 200/200 [00:00&lt;00:00, 394.58 examples/s]"
          }
        },
        "e32a3a178cf642219da252343c2a189e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082bf1d4d46a43e598380be9c8d85f93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647ea4e598694e3582e45e3f8cf5ea41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23caab86142b472eb55f11714ba1be74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9492a19261a34ff7aaf17acc5b6e4d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8248344165ca4294bdce77255215d99b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e462e1e042764266a32160381a599db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vannis422/trainsmall/blob/main/llamatrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装必要依赖（使用transformers最新版）\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q accelerate bitsandbytes\n",
        "\n",
        "# 检查GPU资源\n",
        "import torch\n",
        "print(f\"GPU可用: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU型號: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"內存: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPUGwYl-AaGH",
        "outputId": "5fd4c460-6229-4ca2-cb8e-795a25d1e460"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "GPU可用: True\n",
            "GPU型號: Tesla T4\n",
            "內存: 14.74GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# 下载模型（需要先登录HuggingFace）\n",
        "from huggingface_hub import login\n",
        "login(\"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\")\n",
        "\n",
        "# 下载Llama3-2.1B模型\n",
        "model_path = snapshot_download(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    revision=\"main\",\n",
        "    ignore_patterns=[\"*.bin\", \"*.gguf\"],  # 不下载原始权重，后面会量化\n",
        "    local_dir=\"/content/Llama3-2.1B\"\n",
        ")\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 4-bit量化配置\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 加载并量化模型\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/Llama3-2.1B\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/Llama3-2.1B\")\n",
        "\n",
        "# 保存量化后的模型\n",
        "quant_path = \"/content/Llama3-2.1B-4bit\"\n",
        "model.save_pretrained(quant_path)\n",
        "tokenizer.save_pretrained(quant_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "f19fb44e961a4b388570b4ee4454554e",
            "a23bb55d0e884cb3a7f43f7b8c5a02bb",
            "7ea2396af1284336b5f3ca96108b1d66",
            "d809454885f74cce93deca42cf220131",
            "332dc2fe7865479fafcbbb6b2045aca8",
            "1a52de44af414b378c7e62d43d3bcb75",
            "df5164cb363a466d826e2cff5f046273",
            "f73a0067b51c4726845b9650aa14303a",
            "83c4ed871a1d4ecb80857d6fa1d4ad73",
            "6a2c6570f55d4f5490a52e70f984b167",
            "67be02c54fa845108eb3d4812a79f4a5"
          ]
        },
        "id": "OnRF1lFgBABt",
        "outputId": "b908b8ba-7c58-49e7-f9d2-8877519981d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f19fb44e961a4b388570b4ee4454554e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/Llama3-2.1B-4bit/tokenizer_config.json',\n",
              " '/content/Llama3-2.1B-4bit/special_tokens_map.json',\n",
              " '/content/Llama3-2.1B-4bit/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 安裝必要套件\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "# 載入資料集\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "print(\"✅ 成功載入 PubMedQA 資料集\")\n",
        "\n",
        "# 載入 Llama3.2.1B 模型\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"/content/Llama3-2.1B-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"✅ 成功載入 Llama3.2.1B 模型\")\n",
        "\n",
        "def predict_answer(question, context):\n",
        "    if not isinstance(context, str):\n",
        "        context = \"（無上下文）\"\n",
        "\n",
        "    prompt = f\"\"\"你是一個醫學問答助手。請根據提供的上下文回答問題。\n",
        "\n",
        "    上下文: {context}\n",
        "\n",
        "    問題: {question}\n",
        "\n",
        "    請從以下選項中選擇最合適的答案 (yes/no/maybe):\n",
        "    答案:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"答案:\")[-1].strip().lower()\n",
        "\n",
        "    if \"yes\" in answer:\n",
        "        return \"yes\"\n",
        "    elif \"no\" in answer:\n",
        "        return \"no\"\n",
        "    elif \"maybe\" in answer:\n",
        "        return \"maybe\"\n",
        "    else:\n",
        "        return \"maybe\"\n",
        "\n",
        "print(\"\\n--- 批量測試 100 題 ---\")\n",
        "correct = 0\n",
        "tested = 0\n",
        "max_test = 100\n",
        "i = 0\n",
        "\n",
        "while tested < max_test and i < len(dataset[\"train\"]):\n",
        "    sample = dataset[\"train\"][i]\n",
        "    i += 1\n",
        "\n",
        "    question = sample.get(\"question\", \"\")\n",
        "    context = sample.get(\"context\", \"\")\n",
        "    answer = sample.get(\"final_decision\", \"maybe\")\n",
        "\n",
        "    # 如果 context 不是字串，跳過這題\n",
        "    if not isinstance(context, str):\n",
        "        print(f\"跳過第 {i} 題（無效 context）\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        pred = predict_answer(question, context)\n",
        "        if pred == answer:\n",
        "            correct += 1\n",
        "        tested += 1\n",
        "        print(f\"問題 {tested}: 模型回答={pred}, 正確答案={answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"問題 {i} 處理時出錯: {str(e)}\")\n",
        "\n",
        "if tested > 0:\n",
        "    print(f\"\\n正確率: {correct/tested*100:.2f}% ({correct}/{tested})\")\n",
        "else:\n",
        "    print(\"沒有成功測試任何問題\")\n",
        "\n",
        "print(\"\\n✅ 全部執行完畢！\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLdOUUPaFVTJ",
        "outputId": "5ec5604f-7df8-47f5-fcdd-142a4b770d2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 成功載入 PubMedQA 資料集\n",
            "✅ 成功載入 Llama3.2.1B 模型\n",
            "\n",
            "--- 批量測試 100 題 ---\n",
            "跳過第 1 題（無效 context）\n",
            "跳過第 2 題（無效 context）\n",
            "跳過第 3 題（無效 context）\n",
            "跳過第 4 題（無效 context）\n",
            "跳過第 5 題（無效 context）\n",
            "跳過第 6 題（無效 context）\n",
            "跳過第 7 題（無效 context）\n",
            "跳過第 8 題（無效 context）\n",
            "跳過第 9 題（無效 context）\n",
            "跳過第 10 題（無效 context）\n",
            "跳過第 11 題（無效 context）\n",
            "跳過第 12 題（無效 context）\n",
            "跳過第 13 題（無效 context）\n",
            "跳過第 14 題（無效 context）\n",
            "跳過第 15 題（無效 context）\n",
            "跳過第 16 題（無效 context）\n",
            "跳過第 17 題（無效 context）\n",
            "跳過第 18 題（無效 context）\n",
            "跳過第 19 題（無效 context）\n",
            "跳過第 20 題（無效 context）\n",
            "跳過第 21 題（無效 context）\n",
            "跳過第 22 題（無效 context）\n",
            "跳過第 23 題（無效 context）\n",
            "跳過第 24 題（無效 context）\n",
            "跳過第 25 題（無效 context）\n",
            "跳過第 26 題（無效 context）\n",
            "跳過第 27 題（無效 context）\n",
            "跳過第 28 題（無效 context）\n",
            "跳過第 29 題（無效 context）\n",
            "跳過第 30 題（無效 context）\n",
            "跳過第 31 題（無效 context）\n",
            "跳過第 32 題（無效 context）\n",
            "跳過第 33 題（無效 context）\n",
            "跳過第 34 題（無效 context）\n",
            "跳過第 35 題（無效 context）\n",
            "跳過第 36 題（無效 context）\n",
            "跳過第 37 題（無效 context）\n",
            "跳過第 38 題（無效 context）\n",
            "跳過第 39 題（無效 context）\n",
            "跳過第 40 題（無效 context）\n",
            "跳過第 41 題（無效 context）\n",
            "跳過第 42 題（無效 context）\n",
            "跳過第 43 題（無效 context）\n",
            "跳過第 44 題（無效 context）\n",
            "跳過第 45 題（無效 context）\n",
            "跳過第 46 題（無效 context）\n",
            "跳過第 47 題（無效 context）\n",
            "跳過第 48 題（無效 context）\n",
            "跳過第 49 題（無效 context）\n",
            "跳過第 50 題（無效 context）\n",
            "跳過第 51 題（無效 context）\n",
            "跳過第 52 題（無效 context）\n",
            "跳過第 53 題（無效 context）\n",
            "跳過第 54 題（無效 context）\n",
            "跳過第 55 題（無效 context）\n",
            "跳過第 56 題（無效 context）\n",
            "跳過第 57 題（無效 context）\n",
            "跳過第 58 題（無效 context）\n",
            "跳過第 59 題（無效 context）\n",
            "跳過第 60 題（無效 context）\n",
            "跳過第 61 題（無效 context）\n",
            "跳過第 62 題（無效 context）\n",
            "跳過第 63 題（無效 context）\n",
            "跳過第 64 題（無效 context）\n",
            "跳過第 65 題（無效 context）\n",
            "跳過第 66 題（無效 context）\n",
            "跳過第 67 題（無效 context）\n",
            "跳過第 68 題（無效 context）\n",
            "跳過第 69 題（無效 context）\n",
            "跳過第 70 題（無效 context）\n",
            "跳過第 71 題（無效 context）\n",
            "跳過第 72 題（無效 context）\n",
            "跳過第 73 題（無效 context）\n",
            "跳過第 74 題（無效 context）\n",
            "跳過第 75 題（無效 context）\n",
            "跳過第 76 題（無效 context）\n",
            "跳過第 77 題（無效 context）\n",
            "跳過第 78 題（無效 context）\n",
            "跳過第 79 題（無效 context）\n",
            "跳過第 80 題（無效 context）\n",
            "跳過第 81 題（無效 context）\n",
            "跳過第 82 題（無效 context）\n",
            "跳過第 83 題（無效 context）\n",
            "跳過第 84 題（無效 context）\n",
            "跳過第 85 題（無效 context）\n",
            "跳過第 86 題（無效 context）\n",
            "跳過第 87 題（無效 context）\n",
            "跳過第 88 題（無效 context）\n",
            "跳過第 89 題（無效 context）\n",
            "跳過第 90 題（無效 context）\n",
            "跳過第 91 題（無效 context）\n",
            "跳過第 92 題（無效 context）\n",
            "跳過第 93 題（無效 context）\n",
            "跳過第 94 題（無效 context）\n",
            "跳過第 95 題（無效 context）\n",
            "跳過第 96 題（無效 context）\n",
            "跳過第 97 題（無效 context）\n",
            "跳過第 98 題（無效 context）\n",
            "跳過第 99 題（無效 context）\n",
            "跳過第 100 題（無效 context）\n",
            "跳過第 101 題（無效 context）\n",
            "跳過第 102 題（無效 context）\n",
            "跳過第 103 題（無效 context）\n",
            "跳過第 104 題（無效 context）\n",
            "跳過第 105 題（無效 context）\n",
            "跳過第 106 題（無效 context）\n",
            "跳過第 107 題（無效 context）\n",
            "跳過第 108 題（無效 context）\n",
            "跳過第 109 題（無效 context）\n",
            "跳過第 110 題（無效 context）\n",
            "跳過第 111 題（無效 context）\n",
            "跳過第 112 題（無效 context）\n",
            "跳過第 113 題（無效 context）\n",
            "跳過第 114 題（無效 context）\n",
            "跳過第 115 題（無效 context）\n",
            "跳過第 116 題（無效 context）\n",
            "跳過第 117 題（無效 context）\n",
            "跳過第 118 題（無效 context）\n",
            "跳過第 119 題（無效 context）\n",
            "跳過第 120 題（無效 context）\n",
            "跳過第 121 題（無效 context）\n",
            "跳過第 122 題（無效 context）\n",
            "跳過第 123 題（無效 context）\n",
            "跳過第 124 題（無效 context）\n",
            "跳過第 125 題（無效 context）\n",
            "跳過第 126 題（無效 context）\n",
            "跳過第 127 題（無效 context）\n",
            "跳過第 128 題（無效 context）\n",
            "跳過第 129 題（無效 context）\n",
            "跳過第 130 題（無效 context）\n",
            "跳過第 131 題（無效 context）\n",
            "跳過第 132 題（無效 context）\n",
            "跳過第 133 題（無效 context）\n",
            "跳過第 134 題（無效 context）\n",
            "跳過第 135 題（無效 context）\n",
            "跳過第 136 題（無效 context）\n",
            "跳過第 137 題（無效 context）\n",
            "跳過第 138 題（無效 context）\n",
            "跳過第 139 題（無效 context）\n",
            "跳過第 140 題（無效 context）\n",
            "跳過第 141 題（無效 context）\n",
            "跳過第 142 題（無效 context）\n",
            "跳過第 143 題（無效 context）\n",
            "跳過第 144 題（無效 context）\n",
            "跳過第 145 題（無效 context）\n",
            "跳過第 146 題（無效 context）\n",
            "跳過第 147 題（無效 context）\n",
            "跳過第 148 題（無效 context）\n",
            "跳過第 149 題（無效 context）\n",
            "跳過第 150 題（無效 context）\n",
            "跳過第 151 題（無效 context）\n",
            "跳過第 152 題（無效 context）\n",
            "跳過第 153 題（無效 context）\n",
            "跳過第 154 題（無效 context）\n",
            "跳過第 155 題（無效 context）\n",
            "跳過第 156 題（無效 context）\n",
            "跳過第 157 題（無效 context）\n",
            "跳過第 158 題（無效 context）\n",
            "跳過第 159 題（無效 context）\n",
            "跳過第 160 題（無效 context）\n",
            "跳過第 161 題（無效 context）\n",
            "跳過第 162 題（無效 context）\n",
            "跳過第 163 題（無效 context）\n",
            "跳過第 164 題（無效 context）\n",
            "跳過第 165 題（無效 context）\n",
            "跳過第 166 題（無效 context）\n",
            "跳過第 167 題（無效 context）\n",
            "跳過第 168 題（無效 context）\n",
            "跳過第 169 題（無效 context）\n",
            "跳過第 170 題（無效 context）\n",
            "跳過第 171 題（無效 context）\n",
            "跳過第 172 題（無效 context）\n",
            "跳過第 173 題（無效 context）\n",
            "跳過第 174 題（無效 context）\n",
            "跳過第 175 題（無效 context）\n",
            "跳過第 176 題（無效 context）\n",
            "跳過第 177 題（無效 context）\n",
            "跳過第 178 題（無效 context）\n",
            "跳過第 179 題（無效 context）\n",
            "跳過第 180 題（無效 context）\n",
            "跳過第 181 題（無效 context）\n",
            "跳過第 182 題（無效 context）\n",
            "跳過第 183 題（無效 context）\n",
            "跳過第 184 題（無效 context）\n",
            "跳過第 185 題（無效 context）\n",
            "跳過第 186 題（無效 context）\n",
            "跳過第 187 題（無效 context）\n",
            "跳過第 188 題（無效 context）\n",
            "跳過第 189 題（無效 context）\n",
            "跳過第 190 題（無效 context）\n",
            "跳過第 191 題（無效 context）\n",
            "跳過第 192 題（無效 context）\n",
            "跳過第 193 題（無效 context）\n",
            "跳過第 194 題（無效 context）\n",
            "跳過第 195 題（無效 context）\n",
            "跳過第 196 題（無效 context）\n",
            "跳過第 197 題（無效 context）\n",
            "跳過第 198 題（無效 context）\n",
            "跳過第 199 題（無效 context）\n",
            "跳過第 200 題（無效 context）\n",
            "跳過第 201 題（無效 context）\n",
            "跳過第 202 題（無效 context）\n",
            "跳過第 203 題（無效 context）\n",
            "跳過第 204 題（無效 context）\n",
            "跳過第 205 題（無效 context）\n",
            "跳過第 206 題（無效 context）\n",
            "跳過第 207 題（無效 context）\n",
            "跳過第 208 題（無效 context）\n",
            "跳過第 209 題（無效 context）\n",
            "跳過第 210 題（無效 context）\n",
            "跳過第 211 題（無效 context）\n",
            "跳過第 212 題（無效 context）\n",
            "跳過第 213 題（無效 context）\n",
            "跳過第 214 題（無效 context）\n",
            "跳過第 215 題（無效 context）\n",
            "跳過第 216 題（無效 context）\n",
            "跳過第 217 題（無效 context）\n",
            "跳過第 218 題（無效 context）\n",
            "跳過第 219 題（無效 context）\n",
            "跳過第 220 題（無效 context）\n",
            "跳過第 221 題（無效 context）\n",
            "跳過第 222 題（無效 context）\n",
            "跳過第 223 題（無效 context）\n",
            "跳過第 224 題（無效 context）\n",
            "跳過第 225 題（無效 context）\n",
            "跳過第 226 題（無效 context）\n",
            "跳過第 227 題（無效 context）\n",
            "跳過第 228 題（無效 context）\n",
            "跳過第 229 題（無效 context）\n",
            "跳過第 230 題（無效 context）\n",
            "跳過第 231 題（無效 context）\n",
            "跳過第 232 題（無效 context）\n",
            "跳過第 233 題（無效 context）\n",
            "跳過第 234 題（無效 context）\n",
            "跳過第 235 題（無效 context）\n",
            "跳過第 236 題（無效 context）\n",
            "跳過第 237 題（無效 context）\n",
            "跳過第 238 題（無效 context）\n",
            "跳過第 239 題（無效 context）\n",
            "跳過第 240 題（無效 context）\n",
            "跳過第 241 題（無效 context）\n",
            "跳過第 242 題（無效 context）\n",
            "跳過第 243 題（無效 context）\n",
            "跳過第 244 題（無效 context）\n",
            "跳過第 245 題（無效 context）\n",
            "跳過第 246 題（無效 context）\n",
            "跳過第 247 題（無效 context）\n",
            "跳過第 248 題（無效 context）\n",
            "跳過第 249 題（無效 context）\n",
            "跳過第 250 題（無效 context）\n",
            "跳過第 251 題（無效 context）\n",
            "跳過第 252 題（無效 context）\n",
            "跳過第 253 題（無效 context）\n",
            "跳過第 254 題（無效 context）\n",
            "跳過第 255 題（無效 context）\n",
            "跳過第 256 題（無效 context）\n",
            "跳過第 257 題（無效 context）\n",
            "跳過第 258 題（無效 context）\n",
            "跳過第 259 題（無效 context）\n",
            "跳過第 260 題（無效 context）\n",
            "跳過第 261 題（無效 context）\n",
            "跳過第 262 題（無效 context）\n",
            "跳過第 263 題（無效 context）\n",
            "跳過第 264 題（無效 context）\n",
            "跳過第 265 題（無效 context）\n",
            "跳過第 266 題（無效 context）\n",
            "跳過第 267 題（無效 context）\n",
            "跳過第 268 題（無效 context）\n",
            "跳過第 269 題（無效 context）\n",
            "跳過第 270 題（無效 context）\n",
            "跳過第 271 題（無效 context）\n",
            "跳過第 272 題（無效 context）\n",
            "跳過第 273 題（無效 context）\n",
            "跳過第 274 題（無效 context）\n",
            "跳過第 275 題（無效 context）\n",
            "跳過第 276 題（無效 context）\n",
            "跳過第 277 題（無效 context）\n",
            "跳過第 278 題（無效 context）\n",
            "跳過第 279 題（無效 context）\n",
            "跳過第 280 題（無效 context）\n",
            "跳過第 281 題（無效 context）\n",
            "跳過第 282 題（無效 context）\n",
            "跳過第 283 題（無效 context）\n",
            "跳過第 284 題（無效 context）\n",
            "跳過第 285 題（無效 context）\n",
            "跳過第 286 題（無效 context）\n",
            "跳過第 287 題（無效 context）\n",
            "跳過第 288 題（無效 context）\n",
            "跳過第 289 題（無效 context）\n",
            "跳過第 290 題（無效 context）\n",
            "跳過第 291 題（無效 context）\n",
            "跳過第 292 題（無效 context）\n",
            "跳過第 293 題（無效 context）\n",
            "跳過第 294 題（無效 context）\n",
            "跳過第 295 題（無效 context）\n",
            "跳過第 296 題（無效 context）\n",
            "跳過第 297 題（無效 context）\n",
            "跳過第 298 題（無效 context）\n",
            "跳過第 299 題（無效 context）\n",
            "跳過第 300 題（無效 context）\n",
            "跳過第 301 題（無效 context）\n",
            "跳過第 302 題（無效 context）\n",
            "跳過第 303 題（無效 context）\n",
            "跳過第 304 題（無效 context）\n",
            "跳過第 305 題（無效 context）\n",
            "跳過第 306 題（無效 context）\n",
            "跳過第 307 題（無效 context）\n",
            "跳過第 308 題（無效 context）\n",
            "跳過第 309 題（無效 context）\n",
            "跳過第 310 題（無效 context）\n",
            "跳過第 311 題（無效 context）\n",
            "跳過第 312 題（無效 context）\n",
            "跳過第 313 題（無效 context）\n",
            "跳過第 314 題（無效 context）\n",
            "跳過第 315 題（無效 context）\n",
            "跳過第 316 題（無效 context）\n",
            "跳過第 317 題（無效 context）\n",
            "跳過第 318 題（無效 context）\n",
            "跳過第 319 題（無效 context）\n",
            "跳過第 320 題（無效 context）\n",
            "跳過第 321 題（無效 context）\n",
            "跳過第 322 題（無效 context）\n",
            "跳過第 323 題（無效 context）\n",
            "跳過第 324 題（無效 context）\n",
            "跳過第 325 題（無效 context）\n",
            "跳過第 326 題（無效 context）\n",
            "跳過第 327 題（無效 context）\n",
            "跳過第 328 題（無效 context）\n",
            "跳過第 329 題（無效 context）\n",
            "跳過第 330 題（無效 context）\n",
            "跳過第 331 題（無效 context）\n",
            "跳過第 332 題（無效 context）\n",
            "跳過第 333 題（無效 context）\n",
            "跳過第 334 題（無效 context）\n",
            "跳過第 335 題（無效 context）\n",
            "跳過第 336 題（無效 context）\n",
            "跳過第 337 題（無效 context）\n",
            "跳過第 338 題（無效 context）\n",
            "跳過第 339 題（無效 context）\n",
            "跳過第 340 題（無效 context）\n",
            "跳過第 341 題（無效 context）\n",
            "跳過第 342 題（無效 context）\n",
            "跳過第 343 題（無效 context）\n",
            "跳過第 344 題（無效 context）\n",
            "跳過第 345 題（無效 context）\n",
            "跳過第 346 題（無效 context）\n",
            "跳過第 347 題（無效 context）\n",
            "跳過第 348 題（無效 context）\n",
            "跳過第 349 題（無效 context）\n",
            "跳過第 350 題（無效 context）\n",
            "跳過第 351 題（無效 context）\n",
            "跳過第 352 題（無效 context）\n",
            "跳過第 353 題（無效 context）\n",
            "跳過第 354 題（無效 context）\n",
            "跳過第 355 題（無效 context）\n",
            "跳過第 356 題（無效 context）\n",
            "跳過第 357 題（無效 context）\n",
            "跳過第 358 題（無效 context）\n",
            "跳過第 359 題（無效 context）\n",
            "跳過第 360 題（無效 context）\n",
            "跳過第 361 題（無效 context）\n",
            "跳過第 362 題（無效 context）\n",
            "跳過第 363 題（無效 context）\n",
            "跳過第 364 題（無效 context）\n",
            "跳過第 365 題（無效 context）\n",
            "跳過第 366 題（無效 context）\n",
            "跳過第 367 題（無效 context）\n",
            "跳過第 368 題（無效 context）\n",
            "跳過第 369 題（無效 context）\n",
            "跳過第 370 題（無效 context）\n",
            "跳過第 371 題（無效 context）\n",
            "跳過第 372 題（無效 context）\n",
            "跳過第 373 題（無效 context）\n",
            "跳過第 374 題（無效 context）\n",
            "跳過第 375 題（無效 context）\n",
            "跳過第 376 題（無效 context）\n",
            "跳過第 377 題（無效 context）\n",
            "跳過第 378 題（無效 context）\n",
            "跳過第 379 題（無效 context）\n",
            "跳過第 380 題（無效 context）\n",
            "跳過第 381 題（無效 context）\n",
            "跳過第 382 題（無效 context）\n",
            "跳過第 383 題（無效 context）\n",
            "跳過第 384 題（無效 context）\n",
            "跳過第 385 題（無效 context）\n",
            "跳過第 386 題（無效 context）\n",
            "跳過第 387 題（無效 context）\n",
            "跳過第 388 題（無效 context）\n",
            "跳過第 389 題（無效 context）\n",
            "跳過第 390 題（無效 context）\n",
            "跳過第 391 題（無效 context）\n",
            "跳過第 392 題（無效 context）\n",
            "跳過第 393 題（無效 context）\n",
            "跳過第 394 題（無效 context）\n",
            "跳過第 395 題（無效 context）\n",
            "跳過第 396 題（無效 context）\n",
            "跳過第 397 題（無效 context）\n",
            "跳過第 398 題（無效 context）\n",
            "跳過第 399 題（無效 context）\n",
            "跳過第 400 題（無效 context）\n",
            "跳過第 401 題（無效 context）\n",
            "跳過第 402 題（無效 context）\n",
            "跳過第 403 題（無效 context）\n",
            "跳過第 404 題（無效 context）\n",
            "跳過第 405 題（無效 context）\n",
            "跳過第 406 題（無效 context）\n",
            "跳過第 407 題（無效 context）\n",
            "跳過第 408 題（無效 context）\n",
            "跳過第 409 題（無效 context）\n",
            "跳過第 410 題（無效 context）\n",
            "跳過第 411 題（無效 context）\n",
            "跳過第 412 題（無效 context）\n",
            "跳過第 413 題（無效 context）\n",
            "跳過第 414 題（無效 context）\n",
            "跳過第 415 題（無效 context）\n",
            "跳過第 416 題（無效 context）\n",
            "跳過第 417 題（無效 context）\n",
            "跳過第 418 題（無效 context）\n",
            "跳過第 419 題（無效 context）\n",
            "跳過第 420 題（無效 context）\n",
            "跳過第 421 題（無效 context）\n",
            "跳過第 422 題（無效 context）\n",
            "跳過第 423 題（無效 context）\n",
            "跳過第 424 題（無效 context）\n",
            "跳過第 425 題（無效 context）\n",
            "跳過第 426 題（無效 context）\n",
            "跳過第 427 題（無效 context）\n",
            "跳過第 428 題（無效 context）\n",
            "跳過第 429 題（無效 context）\n",
            "跳過第 430 題（無效 context）\n",
            "跳過第 431 題（無效 context）\n",
            "跳過第 432 題（無效 context）\n",
            "跳過第 433 題（無效 context）\n",
            "跳過第 434 題（無效 context）\n",
            "跳過第 435 題（無效 context）\n",
            "跳過第 436 題（無效 context）\n",
            "跳過第 437 題（無效 context）\n",
            "跳過第 438 題（無效 context）\n",
            "跳過第 439 題（無效 context）\n",
            "跳過第 440 題（無效 context）\n",
            "跳過第 441 題（無效 context）\n",
            "跳過第 442 題（無效 context）\n",
            "跳過第 443 題（無效 context）\n",
            "跳過第 444 題（無效 context）\n",
            "跳過第 445 題（無效 context）\n",
            "跳過第 446 題（無效 context）\n",
            "跳過第 447 題（無效 context）\n",
            "跳過第 448 題（無效 context）\n",
            "跳過第 449 題（無效 context）\n",
            "跳過第 450 題（無效 context）\n",
            "跳過第 451 題（無效 context）\n",
            "跳過第 452 題（無效 context）\n",
            "跳過第 453 題（無效 context）\n",
            "跳過第 454 題（無效 context）\n",
            "跳過第 455 題（無效 context）\n",
            "跳過第 456 題（無效 context）\n",
            "跳過第 457 題（無效 context）\n",
            "跳過第 458 題（無效 context）\n",
            "跳過第 459 題（無效 context）\n",
            "跳過第 460 題（無效 context）\n",
            "跳過第 461 題（無效 context）\n",
            "跳過第 462 題（無效 context）\n",
            "跳過第 463 題（無效 context）\n",
            "跳過第 464 題（無效 context）\n",
            "跳過第 465 題（無效 context）\n",
            "跳過第 466 題（無效 context）\n",
            "跳過第 467 題（無效 context）\n",
            "跳過第 468 題（無效 context）\n",
            "跳過第 469 題（無效 context）\n",
            "跳過第 470 題（無效 context）\n",
            "跳過第 471 題（無效 context）\n",
            "跳過第 472 題（無效 context）\n",
            "跳過第 473 題（無效 context）\n",
            "跳過第 474 題（無效 context）\n",
            "跳過第 475 題（無效 context）\n",
            "跳過第 476 題（無效 context）\n",
            "跳過第 477 題（無效 context）\n",
            "跳過第 478 題（無效 context）\n",
            "跳過第 479 題（無效 context）\n",
            "跳過第 480 題（無效 context）\n",
            "跳過第 481 題（無效 context）\n",
            "跳過第 482 題（無效 context）\n",
            "跳過第 483 題（無效 context）\n",
            "跳過第 484 題（無效 context）\n",
            "跳過第 485 題（無效 context）\n",
            "跳過第 486 題（無效 context）\n",
            "跳過第 487 題（無效 context）\n",
            "跳過第 488 題（無效 context）\n",
            "跳過第 489 題（無效 context）\n",
            "跳過第 490 題（無效 context）\n",
            "跳過第 491 題（無效 context）\n",
            "跳過第 492 題（無效 context）\n",
            "跳過第 493 題（無效 context）\n",
            "跳過第 494 題（無效 context）\n",
            "跳過第 495 題（無效 context）\n",
            "跳過第 496 題（無效 context）\n",
            "跳過第 497 題（無效 context）\n",
            "跳過第 498 題（無效 context）\n",
            "跳過第 499 題（無效 context）\n",
            "跳過第 500 題（無效 context）\n",
            "跳過第 501 題（無效 context）\n",
            "跳過第 502 題（無效 context）\n",
            "跳過第 503 題（無效 context）\n",
            "跳過第 504 題（無效 context）\n",
            "跳過第 505 題（無效 context）\n",
            "跳過第 506 題（無效 context）\n",
            "跳過第 507 題（無效 context）\n",
            "跳過第 508 題（無效 context）\n",
            "跳過第 509 題（無效 context）\n",
            "跳過第 510 題（無效 context）\n",
            "跳過第 511 題（無效 context）\n",
            "跳過第 512 題（無效 context）\n",
            "跳過第 513 題（無效 context）\n",
            "跳過第 514 題（無效 context）\n",
            "跳過第 515 題（無效 context）\n",
            "跳過第 516 題（無效 context）\n",
            "跳過第 517 題（無效 context）\n",
            "跳過第 518 題（無效 context）\n",
            "跳過第 519 題（無效 context）\n",
            "跳過第 520 題（無效 context）\n",
            "跳過第 521 題（無效 context）\n",
            "跳過第 522 題（無效 context）\n",
            "跳過第 523 題（無效 context）\n",
            "跳過第 524 題（無效 context）\n",
            "跳過第 525 題（無效 context）\n",
            "跳過第 526 題（無效 context）\n",
            "跳過第 527 題（無效 context）\n",
            "跳過第 528 題（無效 context）\n",
            "跳過第 529 題（無效 context）\n",
            "跳過第 530 題（無效 context）\n",
            "跳過第 531 題（無效 context）\n",
            "跳過第 532 題（無效 context）\n",
            "跳過第 533 題（無效 context）\n",
            "跳過第 534 題（無效 context）\n",
            "跳過第 535 題（無效 context）\n",
            "跳過第 536 題（無效 context）\n",
            "跳過第 537 題（無效 context）\n",
            "跳過第 538 題（無效 context）\n",
            "跳過第 539 題（無效 context）\n",
            "跳過第 540 題（無效 context）\n",
            "跳過第 541 題（無效 context）\n",
            "跳過第 542 題（無效 context）\n",
            "跳過第 543 題（無效 context）\n",
            "跳過第 544 題（無效 context）\n",
            "跳過第 545 題（無效 context）\n",
            "跳過第 546 題（無效 context）\n",
            "跳過第 547 題（無效 context）\n",
            "跳過第 548 題（無效 context）\n",
            "跳過第 549 題（無效 context）\n",
            "跳過第 550 題（無效 context）\n",
            "跳過第 551 題（無效 context）\n",
            "跳過第 552 題（無效 context）\n",
            "跳過第 553 題（無效 context）\n",
            "跳過第 554 題（無效 context）\n",
            "跳過第 555 題（無效 context）\n",
            "跳過第 556 題（無效 context）\n",
            "跳過第 557 題（無效 context）\n",
            "跳過第 558 題（無效 context）\n",
            "跳過第 559 題（無效 context）\n",
            "跳過第 560 題（無效 context）\n",
            "跳過第 561 題（無效 context）\n",
            "跳過第 562 題（無效 context）\n",
            "跳過第 563 題（無效 context）\n",
            "跳過第 564 題（無效 context）\n",
            "跳過第 565 題（無效 context）\n",
            "跳過第 566 題（無效 context）\n",
            "跳過第 567 題（無效 context）\n",
            "跳過第 568 題（無效 context）\n",
            "跳過第 569 題（無效 context）\n",
            "跳過第 570 題（無效 context）\n",
            "跳過第 571 題（無效 context）\n",
            "跳過第 572 題（無效 context）\n",
            "跳過第 573 題（無效 context）\n",
            "跳過第 574 題（無效 context）\n",
            "跳過第 575 題（無效 context）\n",
            "跳過第 576 題（無效 context）\n",
            "跳過第 577 題（無效 context）\n",
            "跳過第 578 題（無效 context）\n",
            "跳過第 579 題（無效 context）\n",
            "跳過第 580 題（無效 context）\n",
            "跳過第 581 題（無效 context）\n",
            "跳過第 582 題（無效 context）\n",
            "跳過第 583 題（無效 context）\n",
            "跳過第 584 題（無效 context）\n",
            "跳過第 585 題（無效 context）\n",
            "跳過第 586 題（無效 context）\n",
            "跳過第 587 題（無效 context）\n",
            "跳過第 588 題（無效 context）\n",
            "跳過第 589 題（無效 context）\n",
            "跳過第 590 題（無效 context）\n",
            "跳過第 591 題（無效 context）\n",
            "跳過第 592 題（無效 context）\n",
            "跳過第 593 題（無效 context）\n",
            "跳過第 594 題（無效 context）\n",
            "跳過第 595 題（無效 context）\n",
            "跳過第 596 題（無效 context）\n",
            "跳過第 597 題（無效 context）\n",
            "跳過第 598 題（無效 context）\n",
            "跳過第 599 題（無效 context）\n",
            "跳過第 600 題（無效 context）\n",
            "跳過第 601 題（無效 context）\n",
            "跳過第 602 題（無效 context）\n",
            "跳過第 603 題（無效 context）\n",
            "跳過第 604 題（無效 context）\n",
            "跳過第 605 題（無效 context）\n",
            "跳過第 606 題（無效 context）\n",
            "跳過第 607 題（無效 context）\n",
            "跳過第 608 題（無效 context）\n",
            "跳過第 609 題（無效 context）\n",
            "跳過第 610 題（無效 context）\n",
            "跳過第 611 題（無效 context）\n",
            "跳過第 612 題（無效 context）\n",
            "跳過第 613 題（無效 context）\n",
            "跳過第 614 題（無效 context）\n",
            "跳過第 615 題（無效 context）\n",
            "跳過第 616 題（無效 context）\n",
            "跳過第 617 題（無效 context）\n",
            "跳過第 618 題（無效 context）\n",
            "跳過第 619 題（無效 context）\n",
            "跳過第 620 題（無效 context）\n",
            "跳過第 621 題（無效 context）\n",
            "跳過第 622 題（無效 context）\n",
            "跳過第 623 題（無效 context）\n",
            "跳過第 624 題（無效 context）\n",
            "跳過第 625 題（無效 context）\n",
            "跳過第 626 題（無效 context）\n",
            "跳過第 627 題（無效 context）\n",
            "跳過第 628 題（無效 context）\n",
            "跳過第 629 題（無效 context）\n",
            "跳過第 630 題（無效 context）\n",
            "跳過第 631 題（無效 context）\n",
            "跳過第 632 題（無效 context）\n",
            "跳過第 633 題（無效 context）\n",
            "跳過第 634 題（無效 context）\n",
            "跳過第 635 題（無效 context）\n",
            "跳過第 636 題（無效 context）\n",
            "跳過第 637 題（無效 context）\n",
            "跳過第 638 題（無效 context）\n",
            "跳過第 639 題（無效 context）\n",
            "跳過第 640 題（無效 context）\n",
            "跳過第 641 題（無效 context）\n",
            "跳過第 642 題（無效 context）\n",
            "跳過第 643 題（無效 context）\n",
            "跳過第 644 題（無效 context）\n",
            "跳過第 645 題（無效 context）\n",
            "跳過第 646 題（無效 context）\n",
            "跳過第 647 題（無效 context）\n",
            "跳過第 648 題（無效 context）\n",
            "跳過第 649 題（無效 context）\n",
            "跳過第 650 題（無效 context）\n",
            "跳過第 651 題（無效 context）\n",
            "跳過第 652 題（無效 context）\n",
            "跳過第 653 題（無效 context）\n",
            "跳過第 654 題（無效 context）\n",
            "跳過第 655 題（無效 context）\n",
            "跳過第 656 題（無效 context）\n",
            "跳過第 657 題（無效 context）\n",
            "跳過第 658 題（無效 context）\n",
            "跳過第 659 題（無效 context）\n",
            "跳過第 660 題（無效 context）\n",
            "跳過第 661 題（無效 context）\n",
            "跳過第 662 題（無效 context）\n",
            "跳過第 663 題（無效 context）\n",
            "跳過第 664 題（無效 context）\n",
            "跳過第 665 題（無效 context）\n",
            "跳過第 666 題（無效 context）\n",
            "跳過第 667 題（無效 context）\n",
            "跳過第 668 題（無效 context）\n",
            "跳過第 669 題（無效 context）\n",
            "跳過第 670 題（無效 context）\n",
            "跳過第 671 題（無效 context）\n",
            "跳過第 672 題（無效 context）\n",
            "跳過第 673 題（無效 context）\n",
            "跳過第 674 題（無效 context）\n",
            "跳過第 675 題（無效 context）\n",
            "跳過第 676 題（無效 context）\n",
            "跳過第 677 題（無效 context）\n",
            "跳過第 678 題（無效 context）\n",
            "跳過第 679 題（無效 context）\n",
            "跳過第 680 題（無效 context）\n",
            "跳過第 681 題（無效 context）\n",
            "跳過第 682 題（無效 context）\n",
            "跳過第 683 題（無效 context）\n",
            "跳過第 684 題（無效 context）\n",
            "跳過第 685 題（無效 context）\n",
            "跳過第 686 題（無效 context）\n",
            "跳過第 687 題（無效 context）\n",
            "跳過第 688 題（無效 context）\n",
            "跳過第 689 題（無效 context）\n",
            "跳過第 690 題（無效 context）\n",
            "跳過第 691 題（無效 context）\n",
            "跳過第 692 題（無效 context）\n",
            "跳過第 693 題（無效 context）\n",
            "跳過第 694 題（無效 context）\n",
            "跳過第 695 題（無效 context）\n",
            "跳過第 696 題（無效 context）\n",
            "跳過第 697 題（無效 context）\n",
            "跳過第 698 題（無效 context）\n",
            "跳過第 699 題（無效 context）\n",
            "跳過第 700 題（無效 context）\n",
            "跳過第 701 題（無效 context）\n",
            "跳過第 702 題（無效 context）\n",
            "跳過第 703 題（無效 context）\n",
            "跳過第 704 題（無效 context）\n",
            "跳過第 705 題（無效 context）\n",
            "跳過第 706 題（無效 context）\n",
            "跳過第 707 題（無效 context）\n",
            "跳過第 708 題（無效 context）\n",
            "跳過第 709 題（無效 context）\n",
            "跳過第 710 題（無效 context）\n",
            "跳過第 711 題（無效 context）\n",
            "跳過第 712 題（無效 context）\n",
            "跳過第 713 題（無效 context）\n",
            "跳過第 714 題（無效 context）\n",
            "跳過第 715 題（無效 context）\n",
            "跳過第 716 題（無效 context）\n",
            "跳過第 717 題（無效 context）\n",
            "跳過第 718 題（無效 context）\n",
            "跳過第 719 題（無效 context）\n",
            "跳過第 720 題（無效 context）\n",
            "跳過第 721 題（無效 context）\n",
            "跳過第 722 題（無效 context）\n",
            "跳過第 723 題（無效 context）\n",
            "跳過第 724 題（無效 context）\n",
            "跳過第 725 題（無效 context）\n",
            "跳過第 726 題（無效 context）\n",
            "跳過第 727 題（無效 context）\n",
            "跳過第 728 題（無效 context）\n",
            "跳過第 729 題（無效 context）\n",
            "跳過第 730 題（無效 context）\n",
            "跳過第 731 題（無效 context）\n",
            "跳過第 732 題（無效 context）\n",
            "跳過第 733 題（無效 context）\n",
            "跳過第 734 題（無效 context）\n",
            "跳過第 735 題（無效 context）\n",
            "跳過第 736 題（無效 context）\n",
            "跳過第 737 題（無效 context）\n",
            "跳過第 738 題（無效 context）\n",
            "跳過第 739 題（無效 context）\n",
            "跳過第 740 題（無效 context）\n",
            "跳過第 741 題（無效 context）\n",
            "跳過第 742 題（無效 context）\n",
            "跳過第 743 題（無效 context）\n",
            "跳過第 744 題（無效 context）\n",
            "跳過第 745 題（無效 context）\n",
            "跳過第 746 題（無效 context）\n",
            "跳過第 747 題（無效 context）\n",
            "跳過第 748 題（無效 context）\n",
            "跳過第 749 題（無效 context）\n",
            "跳過第 750 題（無效 context）\n",
            "跳過第 751 題（無效 context）\n",
            "跳過第 752 題（無效 context）\n",
            "跳過第 753 題（無效 context）\n",
            "跳過第 754 題（無效 context）\n",
            "跳過第 755 題（無效 context）\n",
            "跳過第 756 題（無效 context）\n",
            "跳過第 757 題（無效 context）\n",
            "跳過第 758 題（無效 context）\n",
            "跳過第 759 題（無效 context）\n",
            "跳過第 760 題（無效 context）\n",
            "跳過第 761 題（無效 context）\n",
            "跳過第 762 題（無效 context）\n",
            "跳過第 763 題（無效 context）\n",
            "跳過第 764 題（無效 context）\n",
            "跳過第 765 題（無效 context）\n",
            "跳過第 766 題（無效 context）\n",
            "跳過第 767 題（無效 context）\n",
            "跳過第 768 題（無效 context）\n",
            "跳過第 769 題（無效 context）\n",
            "跳過第 770 題（無效 context）\n",
            "跳過第 771 題（無效 context）\n",
            "跳過第 772 題（無效 context）\n",
            "跳過第 773 題（無效 context）\n",
            "跳過第 774 題（無效 context）\n",
            "跳過第 775 題（無效 context）\n",
            "跳過第 776 題（無效 context）\n",
            "跳過第 777 題（無效 context）\n",
            "跳過第 778 題（無效 context）\n",
            "跳過第 779 題（無效 context）\n",
            "跳過第 780 題（無效 context）\n",
            "跳過第 781 題（無效 context）\n",
            "跳過第 782 題（無效 context）\n",
            "跳過第 783 題（無效 context）\n",
            "跳過第 784 題（無效 context）\n",
            "跳過第 785 題（無效 context）\n",
            "跳過第 786 題（無效 context）\n",
            "跳過第 787 題（無效 context）\n",
            "跳過第 788 題（無效 context）\n",
            "跳過第 789 題（無效 context）\n",
            "跳過第 790 題（無效 context）\n",
            "跳過第 791 題（無效 context）\n",
            "跳過第 792 題（無效 context）\n",
            "跳過第 793 題（無效 context）\n",
            "跳過第 794 題（無效 context）\n",
            "跳過第 795 題（無效 context）\n",
            "跳過第 796 題（無效 context）\n",
            "跳過第 797 題（無效 context）\n",
            "跳過第 798 題（無效 context）\n",
            "跳過第 799 題（無效 context）\n",
            "跳過第 800 題（無效 context）\n",
            "跳過第 801 題（無效 context）\n",
            "跳過第 802 題（無效 context）\n",
            "跳過第 803 題（無效 context）\n",
            "跳過第 804 題（無效 context）\n",
            "跳過第 805 題（無效 context）\n",
            "跳過第 806 題（無效 context）\n",
            "跳過第 807 題（無效 context）\n",
            "跳過第 808 題（無效 context）\n",
            "跳過第 809 題（無效 context）\n",
            "跳過第 810 題（無效 context）\n",
            "跳過第 811 題（無效 context）\n",
            "跳過第 812 題（無效 context）\n",
            "跳過第 813 題（無效 context）\n",
            "跳過第 814 題（無效 context）\n",
            "跳過第 815 題（無效 context）\n",
            "跳過第 816 題（無效 context）\n",
            "跳過第 817 題（無效 context）\n",
            "跳過第 818 題（無效 context）\n",
            "跳過第 819 題（無效 context）\n",
            "跳過第 820 題（無效 context）\n",
            "跳過第 821 題（無效 context）\n",
            "跳過第 822 題（無效 context）\n",
            "跳過第 823 題（無效 context）\n",
            "跳過第 824 題（無效 context）\n",
            "跳過第 825 題（無效 context）\n",
            "跳過第 826 題（無效 context）\n",
            "跳過第 827 題（無效 context）\n",
            "跳過第 828 題（無效 context）\n",
            "跳過第 829 題（無效 context）\n",
            "跳過第 830 題（無效 context）\n",
            "跳過第 831 題（無效 context）\n",
            "跳過第 832 題（無效 context）\n",
            "跳過第 833 題（無效 context）\n",
            "跳過第 834 題（無效 context）\n",
            "跳過第 835 題（無效 context）\n",
            "跳過第 836 題（無效 context）\n",
            "跳過第 837 題（無效 context）\n",
            "跳過第 838 題（無效 context）\n",
            "跳過第 839 題（無效 context）\n",
            "跳過第 840 題（無效 context）\n",
            "跳過第 841 題（無效 context）\n",
            "跳過第 842 題（無效 context）\n",
            "跳過第 843 題（無效 context）\n",
            "跳過第 844 題（無效 context）\n",
            "跳過第 845 題（無效 context）\n",
            "跳過第 846 題（無效 context）\n",
            "跳過第 847 題（無效 context）\n",
            "跳過第 848 題（無效 context）\n",
            "跳過第 849 題（無效 context）\n",
            "跳過第 850 題（無效 context）\n",
            "跳過第 851 題（無效 context）\n",
            "跳過第 852 題（無效 context）\n",
            "跳過第 853 題（無效 context）\n",
            "跳過第 854 題（無效 context）\n",
            "跳過第 855 題（無效 context）\n",
            "跳過第 856 題（無效 context）\n",
            "跳過第 857 題（無效 context）\n",
            "跳過第 858 題（無效 context）\n",
            "跳過第 859 題（無效 context）\n",
            "跳過第 860 題（無效 context）\n",
            "跳過第 861 題（無效 context）\n",
            "跳過第 862 題（無效 context）\n",
            "跳過第 863 題（無效 context）\n",
            "跳過第 864 題（無效 context）\n",
            "跳過第 865 題（無效 context）\n",
            "跳過第 866 題（無效 context）\n",
            "跳過第 867 題（無效 context）\n",
            "跳過第 868 題（無效 context）\n",
            "跳過第 869 題（無效 context）\n",
            "跳過第 870 題（無效 context）\n",
            "跳過第 871 題（無效 context）\n",
            "跳過第 872 題（無效 context）\n",
            "跳過第 873 題（無效 context）\n",
            "跳過第 874 題（無效 context）\n",
            "跳過第 875 題（無效 context）\n",
            "跳過第 876 題（無效 context）\n",
            "跳過第 877 題（無效 context）\n",
            "跳過第 878 題（無效 context）\n",
            "跳過第 879 題（無效 context）\n",
            "跳過第 880 題（無效 context）\n",
            "跳過第 881 題（無效 context）\n",
            "跳過第 882 題（無效 context）\n",
            "跳過第 883 題（無效 context）\n",
            "跳過第 884 題（無效 context）\n",
            "跳過第 885 題（無效 context）\n",
            "跳過第 886 題（無效 context）\n",
            "跳過第 887 題（無效 context）\n",
            "跳過第 888 題（無效 context）\n",
            "跳過第 889 題（無效 context）\n",
            "跳過第 890 題（無效 context）\n",
            "跳過第 891 題（無效 context）\n",
            "跳過第 892 題（無效 context）\n",
            "跳過第 893 題（無效 context）\n",
            "跳過第 894 題（無效 context）\n",
            "跳過第 895 題（無效 context）\n",
            "跳過第 896 題（無效 context）\n",
            "跳過第 897 題（無效 context）\n",
            "跳過第 898 題（無效 context）\n",
            "跳過第 899 題（無效 context）\n",
            "跳過第 900 題（無效 context）\n",
            "跳過第 901 題（無效 context）\n",
            "跳過第 902 題（無效 context）\n",
            "跳過第 903 題（無效 context）\n",
            "跳過第 904 題（無效 context）\n",
            "跳過第 905 題（無效 context）\n",
            "跳過第 906 題（無效 context）\n",
            "跳過第 907 題（無效 context）\n",
            "跳過第 908 題（無效 context）\n",
            "跳過第 909 題（無效 context）\n",
            "跳過第 910 題（無效 context）\n",
            "跳過第 911 題（無效 context）\n",
            "跳過第 912 題（無效 context）\n",
            "跳過第 913 題（無效 context）\n",
            "跳過第 914 題（無效 context）\n",
            "跳過第 915 題（無效 context）\n",
            "跳過第 916 題（無效 context）\n",
            "跳過第 917 題（無效 context）\n",
            "跳過第 918 題（無效 context）\n",
            "跳過第 919 題（無效 context）\n",
            "跳過第 920 題（無效 context）\n",
            "跳過第 921 題（無效 context）\n",
            "跳過第 922 題（無效 context）\n",
            "跳過第 923 題（無效 context）\n",
            "跳過第 924 題（無效 context）\n",
            "跳過第 925 題（無效 context）\n",
            "跳過第 926 題（無效 context）\n",
            "跳過第 927 題（無效 context）\n",
            "跳過第 928 題（無效 context）\n",
            "跳過第 929 題（無效 context）\n",
            "跳過第 930 題（無效 context）\n",
            "跳過第 931 題（無效 context）\n",
            "跳過第 932 題（無效 context）\n",
            "跳過第 933 題（無效 context）\n",
            "跳過第 934 題（無效 context）\n",
            "跳過第 935 題（無效 context）\n",
            "跳過第 936 題（無效 context）\n",
            "跳過第 937 題（無效 context）\n",
            "跳過第 938 題（無效 context）\n",
            "跳過第 939 題（無效 context）\n",
            "跳過第 940 題（無效 context）\n",
            "跳過第 941 題（無效 context）\n",
            "跳過第 942 題（無效 context）\n",
            "跳過第 943 題（無效 context）\n",
            "跳過第 944 題（無效 context）\n",
            "跳過第 945 題（無效 context）\n",
            "跳過第 946 題（無效 context）\n",
            "跳過第 947 題（無效 context）\n",
            "跳過第 948 題（無效 context）\n",
            "跳過第 949 題（無效 context）\n",
            "跳過第 950 題（無效 context）\n",
            "跳過第 951 題（無效 context）\n",
            "跳過第 952 題（無效 context）\n",
            "跳過第 953 題（無效 context）\n",
            "跳過第 954 題（無效 context）\n",
            "跳過第 955 題（無效 context）\n",
            "跳過第 956 題（無效 context）\n",
            "跳過第 957 題（無效 context）\n",
            "跳過第 958 題（無效 context）\n",
            "跳過第 959 題（無效 context）\n",
            "跳過第 960 題（無效 context）\n",
            "跳過第 961 題（無效 context）\n",
            "跳過第 962 題（無效 context）\n",
            "跳過第 963 題（無效 context）\n",
            "跳過第 964 題（無效 context）\n",
            "跳過第 965 題（無效 context）\n",
            "跳過第 966 題（無效 context）\n",
            "跳過第 967 題（無效 context）\n",
            "跳過第 968 題（無效 context）\n",
            "跳過第 969 題（無效 context）\n",
            "跳過第 970 題（無效 context）\n",
            "跳過第 971 題（無效 context）\n",
            "跳過第 972 題（無效 context）\n",
            "跳過第 973 題（無效 context）\n",
            "跳過第 974 題（無效 context）\n",
            "跳過第 975 題（無效 context）\n",
            "跳過第 976 題（無效 context）\n",
            "跳過第 977 題（無效 context）\n",
            "跳過第 978 題（無效 context）\n",
            "跳過第 979 題（無效 context）\n",
            "跳過第 980 題（無效 context）\n",
            "跳過第 981 題（無效 context）\n",
            "跳過第 982 題（無效 context）\n",
            "跳過第 983 題（無效 context）\n",
            "跳過第 984 題（無效 context）\n",
            "跳過第 985 題（無效 context）\n",
            "跳過第 986 題（無效 context）\n",
            "跳過第 987 題（無效 context）\n",
            "跳過第 988 題（無效 context）\n",
            "跳過第 989 題（無效 context）\n",
            "跳過第 990 題（無效 context）\n",
            "跳過第 991 題（無效 context）\n",
            "跳過第 992 題（無效 context）\n",
            "跳過第 993 題（無效 context）\n",
            "跳過第 994 題（無效 context）\n",
            "跳過第 995 題（無效 context）\n",
            "跳過第 996 題（無效 context）\n",
            "跳過第 997 題（無效 context）\n",
            "跳過第 998 題（無效 context）\n",
            "跳過第 999 題（無效 context）\n",
            "跳過第 1000 題（無效 context）\n",
            "沒有成功測試任何問題\n",
            "\n",
            "✅ 全部執行完畢！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 安裝必要套件（如已安裝可跳過）\n",
        "!pip install -q transformers datasets bitsandbytes accelerate\n",
        "\n",
        "# ✅ 載入必要模組\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# 載入 PubMedQA 資料集\n",
        "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "\n",
        "# ✅ 使用 .select + .to_dict 取得 list of dicts（真正安全）\n",
        "raw_subset = dataset[\"train\"].select(range(min(100, len(dataset[\"train\"]))))\n",
        "samples_to_test = [dict(x) for x in raw_subset]\n",
        "\n",
        "print(f\"✅ 測試樣本數量：{len(samples_to_test)} 題\")\n",
        "\n",
        "# ✅ 設定模型與 tokenizer（請依你的實際路徑修改）\n",
        "model_path = \"/content/Llama3-2.1B-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"✅ 模型載入完成\")\n",
        "\n",
        "# ✅ 定義推理函數（不使用 context）\n",
        "def predict_answer(question):\n",
        "    prompt = f\"\"\"你是一個醫學問答助手。請根據你的知識回答下列問題。\n",
        "\n",
        "    問題: {question}\n",
        "\n",
        "    請從以下選項中選擇最合適的答案 (yes/no/maybe):\n",
        "    答案:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"答案:\")[-1].strip().lower()\n",
        "\n",
        "    if \"yes\" in answer:\n",
        "        return \"yes\"\n",
        "    elif \"no\" in answer:\n",
        "        return \"no\"\n",
        "    elif \"maybe\" in answer:\n",
        "        return \"maybe\"\n",
        "    else:\n",
        "        return \"maybe\"  # 預設保守答案\n",
        "\n",
        "# ✅ 測試前 100 題並計算正確率\n",
        "correct = 0\n",
        "total = len(samples_to_test)\n",
        "\n",
        "print(\"\\n--- 測試開始 ---\\n\")\n",
        "\n",
        "for i, sample in enumerate(samples_to_test):\n",
        "    question = sample[\"question\"]\n",
        "    true_answer = sample[\"final_decision\"]\n",
        "\n",
        "    try:\n",
        "        pred = predict_answer(question)\n",
        "        result = \"✅\" if pred == true_answer else \"❌\"\n",
        "        if pred == true_answer:\n",
        "            correct += 1\n",
        "        print(f\"{result} 第 {i+1} 題：模型回答 = {pred}, 正確答案 = {true_answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 第 {i+1} 題出錯：{e}\")\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "print(f\"\\n🎯 測試完成！模型純知識準確率：{accuracy:.2f}%（{correct}/{total}）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb31ICqIL5cd",
        "outputId": "c5d58587-53f6-4a34-8cfa-f1cdf1620fcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 測試樣本數量：100 題\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 模型載入完成\n",
            "\n",
            "--- 測試開始 ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 1 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 2 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 3 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 4 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 5 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 6 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 7 題：模型回答 = maybe, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 8 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 9 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 10 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 11 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 12 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 13 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 14 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 15 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 16 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 17 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 18 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 19 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 20 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 21 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 22 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 23 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 24 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 25 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 26 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 27 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 28 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 29 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 30 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 31 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 32 題：模型回答 = no, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 33 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 34 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 35 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 36 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 37 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 38 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 39 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 40 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 41 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 42 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 43 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 44 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 45 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 46 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 47 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 48 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 49 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 50 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 51 題：模型回答 = maybe, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 52 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 53 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 54 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 55 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 56 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 57 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 58 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 59 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 60 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 61 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 62 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 63 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 64 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 65 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 66 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 67 題：模型回答 = no, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 68 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 69 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 70 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 71 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 72 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 73 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 74 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 75 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 76 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 77 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 78 題：模型回答 = maybe, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 79 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 80 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 81 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 82 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 83 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 84 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 85 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 86 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 87 題：模型回答 = maybe, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 88 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 89 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 90 題：模型回答 = yes, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 91 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 92 題：模型回答 = no, 正確答案 = maybe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 93 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 94 題：模型回答 = maybe, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 95 題：模型回答 = yes, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 96 題：模型回答 = maybe, 正確答案 = no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 97 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 第 98 題：模型回答 = yes, 正確答案 = yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 第 99 題：模型回答 = yes, 正確答案 = no\n",
            "✅ 第 100 題：模型回答 = yes, 正確答案 = yes\n",
            "\n",
            "🎯 測試完成！模型純知識準確率：41.00%（41/100）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#r=8\n",
        "!pip install -q peft transformers accelerate bitsandbytes datasets evaluate\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. 載入數據集\n",
        "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"train\"]\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_val_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "train_val_split = train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "train_dataset = train_val_split[\"train\"]\n",
        "val_dataset = train_val_split[\"test\"]\n",
        "\n",
        "# 2. 設置標籤映射\n",
        "label2id = {\"yes\": 0, \"no\": 1, \"maybe\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# 3. 載入tokenizer並正確設置pad token\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/Llama3-2.1B-4bit\")\n",
        "\n",
        "# 重要修正：確保pad token設置正確\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# 同时需要更新模型配置\n",
        "if hasattr(tokenizer, 'pad_token_id'):\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 4. 數據預處理（修正版）\n",
        "def preprocess(examples):\n",
        "    prompts = [\n",
        "        f\"Question: {q}\\nContext: {c}\\nAnswer:\"\n",
        "        for q, c in zip(examples[\"question\"], examples[\"context\"])\n",
        "    ]\n",
        "    encodings = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        padding=False,  # 改為False，由DataCollator處理\n",
        "        max_length=512,\n",
        "        return_tensors=None  # 返回普通字典而非tensors\n",
        "    )\n",
        "    encodings[\"labels\"] = [label2id[label] for label in examples[\"final_decision\"]]\n",
        "    return encodings\n",
        "\n",
        "# 應用預處理\n",
        "train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)\n",
        "test_dataset = test_dataset.map(preprocess, batched=True, remove_columns=test_dataset.column_names)\n",
        "\n",
        "# 5. 創建數據收集器（處理padding）\n",
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# 6. 載入量化模型（不再傳入quantization_config）\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/Llama3-2.1B-4bit\",\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# 关键修正：确保模型配置有pad_token_id\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 7. 準備模型進行k-bit訓練\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 8. 配置LoRA參數\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "# 9. 添加LoRA適配器\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 10. 修正訓練參數（使用正確的參數名稱）(用pytorch操作)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # 正確的參數名稱\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=3,\n",
        "    gradient_checkpointing=True,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "# 11. 評估指標\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "# 12. 初始化Trainer(改成pytorch)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 13. 開始訓練\n",
        "trainer.train()\n",
        "\n",
        "# 14. 評估模型\n",
        "val_result = trainer.evaluate()\n",
        "print(f\"\\nValidation Accuracy: {val_result['eval_accuracy']:.2%}\")\n",
        "\n",
        "test_result = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"\\nTest Accuracy: {test_result['eval_accuracy']:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "a6e1202a5bde498fbc668a608e4397f1",
            "de8b9fe6efd44e4ebdb1a1606692e7ed",
            "dd479ca6167e4a7f85b2dc1f4f0f4401",
            "e2401253fb464a6497c621e18777ade4",
            "e32a3a178cf642219da252343c2a189e",
            "082bf1d4d46a43e598380be9c8d85f93",
            "647ea4e598694e3582e45e3f8cf5ea41",
            "23caab86142b472eb55f11714ba1be74",
            "9492a19261a34ff7aaf17acc5b6e4d62",
            "8248344165ca4294bdce77255215d99b",
            "e462e1e042764266a32160381a599db8"
          ]
        },
        "id": "3xhFa_F3G5GU",
        "outputId": "216e19bf-e6de-4a40-8efa-305dce256b94"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6e1202a5bde498fbc668a608e4397f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /content/Llama3-2.1B-4bit and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-19-fb488acb8f95>:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 858,112 || all params: 1,236,678,656 || trainable%: 0.0694\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/220 16:49, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.470625</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.516284</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.501726</td>\n",
              "      <td>0.780000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.502044</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.493873</td>\n",
              "      <td>0.780000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Accuracy: 78.00%\n",
            "\n",
            "Test Accuracy: 79.00%\n"
          ]
        }
      ]
    }
  ]
}