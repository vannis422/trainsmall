# -*- coding: utf-8 -*-
"""LLMmodule

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-9PINbbM5aqvfa7igefA2HYreLiBzPOP
"""

from huggingface_hub import login, snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# 外掛：登入
def HuggingFaceAuth(token: str):
    login("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")

# 外掛：下載模型
def ModelDownloader(repo_id: str, local_dir: str, revision="main", ignore_patterns=None):
    return snapshot_download(
        repo_id,
        revision=revision,
        ignore_patterns=ignore_patterns or [],
        local_dir=local_dir
    )

# 外掛：建立量化設定
def QuantConfigBuilder(dtype=torch.bfloat16, quant_type="nf4", use_double_quant=True):
    return BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type=quant_type,
        bnb_4bit_use_double_quant=use_double_quant,
        bnb_4bit_compute_dtype=dtype
    )

# 外掛：載入量化模型
def QuantizedModelLoader(model_path: str, quant_config, device="auto", dtype=torch.bfloat16):
    return AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quant_config,
        device_map=device,
        torch_dtype=dtype
    )

# 外掛：儲存模型
def ModelSaver(model, tokenizer, save_path: str):
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)